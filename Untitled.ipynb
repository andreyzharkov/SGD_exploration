{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.layers import Input, Dense, Lambda, Conv2D, Flatten, Activation, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model, Sequential\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "import numpy as np\n",
    "import os\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import SGDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 12s 1us/step\n",
      "X_train original shape (60000, 28, 28)\n",
      "y_train original shape (60000,)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "\n",
    "# the data, shuffled and split between tran and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(\"X_train original shape\", X_train.shape)\n",
    "print(\"y_train original shape\", y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGDL(SGD):\n",
    "    \"\"\"Stochastic gradient descent Langevian optimizer.\n",
    "\n",
    "    Includes support for momentum,\n",
    "    learning rate decay, and Nesterov momentum.\n",
    "\n",
    "    # Arguments\n",
    "        lr: float >= 0. Learning rate.\n",
    "        momentum: float >= 0. Parameter that accelerates SGD\n",
    "            in the relevant direction and dampens oscillations.\n",
    "        decay: float >= 0. Learning rate decay over each update.\n",
    "        nesterov: boolean. Whether to apply Nesterov momentum.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, lr=0.001, gamma=0.1, **kwargs):\n",
    "        super(SGDL, self).__init__(**kwargs)\n",
    "        with K.name_scope(self.__class__.__name__):\n",
    "            self.gamma = K.variable(0.1, name=\"gamma\")\n",
    "        print(K.int_shape(self.iterations))\n",
    "\n",
    "    def get_updates(self, loss, params):\n",
    "        grads = self.get_gradients(loss, params)\n",
    "        self.updates = [K.update_add(self.iterations, 1)]\n",
    "\n",
    "        lr = self.lr\n",
    "        if self.initial_decay > 0:\n",
    "            lr = lr * (1. / (1. + self.decay * K.cast(self.iterations,\n",
    "                                                      K.dtype(self.decay))))\n",
    "\n",
    "        gamma = self.gamma * (1. / (1. + K.cast(self.iterations, tf.float32)))\n",
    "\n",
    "        # momentum\n",
    "        shapes = [K.int_shape(p) for p in params]\n",
    "        gaussians = [K.random_normal(shape) for shape in shapes]\n",
    "        moments = [K.zeros(shape) for shape in shapes]\n",
    "        self.weights = [self.iterations] + moments\n",
    "        for p, g, m, W in zip(params, grads, moments, gaussians):\n",
    "            v = self.momentum * m - lr * g  # velocity\n",
    "            self.updates.append(K.update(m, v))\n",
    "\n",
    "            if self.nesterov:\n",
    "                new_p = p + self.momentum * v - lr * g\n",
    "            else:\n",
    "                new_p = p + v\n",
    "            new_p = new_p + gamma * W\n",
    "\n",
    "            # Apply constraints.\n",
    "            if getattr(p, 'constraint', None) is not None:\n",
    "                new_p = p.constraint(new_p)\n",
    "\n",
    "            self.updates.append(K.update(p, new_p))\n",
    "        return self.updates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/3\n",
      "60000/60000 [==============================] - 4s 75us/step - loss: 0.6451 - acc: 0.8236 - val_loss: 0.3002 - val_acc: 0.9153\n",
      "Epoch 2/3\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.3224 - acc: 0.9062 - val_loss: 0.2353 - val_acc: 0.9316\n",
      "Epoch 3/3\n",
      "60000/60000 [==============================] - 4s 62us/step - loss: 0.2588 - acc: 0.9252 - val_loss: 0.1950 - val_acc: 0.9451\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f1f2fcf1588>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_model():\n",
    "#     inp = Input(shape=(28, 28))\n",
    "#     x = inp\n",
    "# #     x = Lambda(lambda x: K.expand_dims(x, -1))(x)\n",
    "# #     x = Conv2D(32, (5, 5), activation='relu')(x)\n",
    "# #     x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "# #     x = Conv2D(32, (3, 3), activation='relu')(x)\n",
    "#     x = Flatten()(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dense(100, activation='relu')(x)\n",
    "#     x = Dense(10, activation='softmax')(x)\n",
    "#     model = Model(inputs=inp, outputs=x)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Lambda(lambda x: x / 255.))\n",
    "    model.add(Dense(512, input_shape=(784,)))\n",
    "    model.add(Activation('relu')) # An \"activation\" is just a non-linear function applied to the output\n",
    "                                  # of the layer above. Here, with a \"rectified linear unit\",\n",
    "                                  # we clamp all values below 0 to 0.\n",
    "\n",
    "    model.add(Dropout(0.2))   # Dropout helps protect the model from memorizing or \"overfitting\" the training data\n",
    "    model.add(Dense(512))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(10))\n",
    "    model.add(Activation('softmax')) # This special \"softmax\" activation among other things,\n",
    "                                     # ensures the output is a valid probaility distribution, that is\n",
    "                                     # that its values are all non-negative and sum to 1.\n",
    "    return model\n",
    "\n",
    "model = build_model()\n",
    "model.compile(optimizer=SGD(), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(X_train, y_train,\n",
    "          batch_size=32, epochs=3,\n",
    "          verbose=1,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "()\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5711 - acc: 0.8217 - val_loss: 0.2381 - val_acc: 0.9330\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.3007 - acc: 0.9086 - val_loss: 0.1888 - val_acc: 0.9436\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2405 - acc: 0.9267 - val_loss: 0.1592 - val_acc: 0.9519\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2072 - acc: 0.9372 - val_loss: 0.1418 - val_acc: 0.9556\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.1828 - acc: 0.9446 - val_loss: 0.1307 - val_acc: 0.9595\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1623 - acc: 0.9506 - val_loss: 0.1189 - val_acc: 0.9631\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1513 - acc: 0.9543 - val_loss: 0.1113 - val_acc: 0.9656\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1400 - acc: 0.9570 - val_loss: 0.1048 - val_acc: 0.9670\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1298 - acc: 0.9601 - val_loss: 0.0998 - val_acc: 0.9685\n",
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1219 - acc: 0.9617 - val_loss: 0.0964 - val_acc: 0.9691\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1151 - acc: 0.9648 - val_loss: 0.0919 - val_acc: 0.9715\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1089 - acc: 0.9666 - val_loss: 0.0904 - val_acc: 0.9714\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1014 - acc: 0.9683 - val_loss: 0.0859 - val_acc: 0.9730\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0986 - acc: 0.9696 - val_loss: 0.0835 - val_acc: 0.9742\n",
      "Epoch 15/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0942 - acc: 0.9708 - val_loss: 0.0817 - val_acc: 0.9741\n",
      "Epoch 16/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0878 - acc: 0.9725 - val_loss: 0.0797 - val_acc: 0.9749\n",
      "Epoch 17/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0844 - acc: 0.9735 - val_loss: 0.0794 - val_acc: 0.9739\n",
      "Epoch 18/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0843 - acc: 0.9732 - val_loss: 0.0764 - val_acc: 0.9749\n",
      "Epoch 19/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0792 - acc: 0.9748 - val_loss: 0.0761 - val_acc: 0.9764\n",
      "Epoch 20/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0763 - acc: 0.9763 - val_loss: 0.0751 - val_acc: 0.9761\n",
      "Epoch 21/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0748 - acc: 0.9766 - val_loss: 0.0732 - val_acc: 0.9765\n",
      "Epoch 22/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0693 - acc: 0.9782 - val_loss: 0.0726 - val_acc: 0.9765\n",
      "Epoch 23/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0666 - acc: 0.9789 - val_loss: 0.0713 - val_acc: 0.9767\n",
      "Epoch 24/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0652 - acc: 0.9796 - val_loss: 0.0707 - val_acc: 0.9773\n",
      "Epoch 25/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0633 - acc: 0.9803 - val_loss: 0.0694 - val_acc: 0.9771\n",
      "Epoch 26/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0616 - acc: 0.9805 - val_loss: 0.0678 - val_acc: 0.9780\n",
      "Epoch 27/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0592 - acc: 0.9813 - val_loss: 0.0676 - val_acc: 0.9781\n",
      "Epoch 28/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0577 - acc: 0.9813 - val_loss: 0.0674 - val_acc: 0.9783\n",
      "Epoch 29/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0560 - acc: 0.9817 - val_loss: 0.0666 - val_acc: 0.9788\n",
      "Epoch 30/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0544 - acc: 0.9825 - val_loss: 0.0650 - val_acc: 0.9791\n",
      "Epoch 31/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0514 - acc: 0.9829 - val_loss: 0.0641 - val_acc: 0.9795\n",
      "Epoch 32/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0521 - acc: 0.9830 - val_loss: 0.0646 - val_acc: 0.9792\n",
      "Epoch 33/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0494 - acc: 0.9840 - val_loss: 0.0633 - val_acc: 0.9798\n",
      "Epoch 34/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0468 - acc: 0.9853 - val_loss: 0.0641 - val_acc: 0.9789\n",
      "Epoch 35/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0469 - acc: 0.9852 - val_loss: 0.0636 - val_acc: 0.9795\n",
      "Epoch 36/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0441 - acc: 0.9859 - val_loss: 0.0625 - val_acc: 0.9808\n",
      "Epoch 37/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0452 - acc: 0.9852 - val_loss: 0.0622 - val_acc: 0.9801\n",
      "Epoch 38/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0430 - acc: 0.9864 - val_loss: 0.0627 - val_acc: 0.9801\n",
      "Epoch 39/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0414 - acc: 0.9872 - val_loss: 0.0619 - val_acc: 0.9805\n",
      "Epoch 40/50\n",
      "60000/60000 [==============================] - 4s 64us/step - loss: 0.0415 - acc: 0.9863 - val_loss: 0.0615 - val_acc: 0.9811\n",
      "Epoch 41/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0383 - acc: 0.9875 - val_loss: 0.0613 - val_acc: 0.9807\n",
      "Epoch 42/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0395 - acc: 0.9873 - val_loss: 0.0623 - val_acc: 0.9810\n",
      "Epoch 43/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0366 - acc: 0.9882 - val_loss: 0.0610 - val_acc: 0.9811\n",
      "Epoch 44/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0363 - acc: 0.9887 - val_loss: 0.0601 - val_acc: 0.9807\n",
      "Epoch 45/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0355 - acc: 0.9887 - val_loss: 0.0604 - val_acc: 0.9818\n",
      "Epoch 46/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0350 - acc: 0.9883 - val_loss: 0.0612 - val_acc: 0.9814\n",
      "Epoch 47/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0333 - acc: 0.9897 - val_loss: 0.0613 - val_acc: 0.9820\n",
      "Epoch 48/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0331 - acc: 0.9899 - val_loss: 0.0604 - val_acc: 0.9816\n",
      "Epoch 49/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0314 - acc: 0.9900 - val_loss: 0.0608 - val_acc: 0.9811\n",
      "Epoch 50/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0323 - acc: 0.9898 - val_loss: 0.0604 - val_acc: 0.9816\n",
      "()\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "60000/60000 [==============================] - 5s 80us/step - loss: 0.5686 - acc: 0.8227 - val_loss: 0.2417 - val_acc: 0.9289\n",
      "Epoch 2/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2966 - acc: 0.9108 - val_loss: 0.1834 - val_acc: 0.9467\n",
      "Epoch 3/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2377 - acc: 0.9276 - val_loss: 0.1564 - val_acc: 0.9536\n",
      "Epoch 4/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.2024 - acc: 0.9391 - val_loss: 0.1393 - val_acc: 0.9577\n",
      "Epoch 5/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1799 - acc: 0.9449 - val_loss: 0.1271 - val_acc: 0.9614\n",
      "Epoch 6/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1640 - acc: 0.9502 - val_loss: 0.1168 - val_acc: 0.9645\n",
      "Epoch 7/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1482 - acc: 0.9547 - val_loss: 0.1101 - val_acc: 0.9662\n",
      "Epoch 8/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1376 - acc: 0.9583 - val_loss: 0.1035 - val_acc: 0.9681\n",
      "Epoch 9/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1288 - acc: 0.9607 - val_loss: 0.0992 - val_acc: 0.9700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1203 - acc: 0.9641 - val_loss: 0.0942 - val_acc: 0.9714\n",
      "Epoch 11/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1125 - acc: 0.9664 - val_loss: 0.0907 - val_acc: 0.9723\n",
      "Epoch 12/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.1057 - acc: 0.9672 - val_loss: 0.0891 - val_acc: 0.9730\n",
      "Epoch 13/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0985 - acc: 0.9698 - val_loss: 0.0855 - val_acc: 0.9738\n",
      "Epoch 14/50\n",
      "60000/60000 [==============================] - 4s 65us/step - loss: 0.0967 - acc: 0.9700 - val_loss: 0.0829 - val_acc: 0.9747\n",
      "Epoch 15/50\n",
      " 6720/60000 [==>...........................] - ETA: 3s - loss: 0.0923 - acc: 0.9719"
     ]
    }
   ],
   "source": [
    "os.makedirs('logs', exist_ok=True)\n",
    "for gamma in [0, 0.1, 0.01, 0.001, 0.0001]:\n",
    "    cb = keras.callbacks.CSVLogger(f'logs/SGDL_{gamma}.txt', separator=',')\n",
    "    model = build_model()\n",
    "    model.compile(optimizer=SGDL(gamma=gamma), loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X_train, y_train,\n",
    "              batch_size=32, epochs=50,\n",
    "              verbose=1,\n",
    "              validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
